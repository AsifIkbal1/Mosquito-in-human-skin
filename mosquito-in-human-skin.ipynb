{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport tensorflow \nimport pathlib\nimport numpy as np\nimport os\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# View an image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport random\n\n# read examples of images\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n#preprocessing\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the class names (programmatically, this is much more helpful with a longer list of classes)\ndata_dir = pathlib.Path(\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Train/\") # turn our training path into a Python path\nclass_names = np.array(sorted([item.name for item in data_dir.glob('*')])) # created a list of class_names from the subdirectories\nprint(class_names)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:47:59.666025Z","iopub.execute_input":"2022-08-03T03:47:59.666871Z","iopub.status.idle":"2022-08-03T03:47:59.69644Z","shell.execute_reply.started":"2022-08-03T03:47:59.666842Z","shell.execute_reply":"2022-08-03T03:47:59.694156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Walk through directory and list number of files\nfor dirpath, dirnames, filenames in os.walk(\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/\"):\n  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:54:03.218892Z","iopub.execute_input":"2022-08-03T04:54:03.219601Z","iopub.status.idle":"2022-08-03T04:54:03.264285Z","shell.execute_reply.started":"2022-08-03T04:54:03.219562Z","shell.execute_reply":"2022-08-03T04:54:03.263259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 6 classes that are consisist of `Culex landing, Culex smashed, aegypti landing, aegypti smashed, albopictus landing, albopictus smashed` and 700 training images and 300 testing images of each class.","metadata":{}},{"cell_type":"code","source":"def view_random_image(target_dir, target_class):\n    # Setup target directory (we'll view images from here)\n    target_folder = target_dir+target_class\n\n    # Get a random image path\n    random_image = random.sample(os.listdir(target_folder), 1)\n\n    # Read in the image and plot it using matplotlib\n    img = mpimg.imread(target_folder + \"/\" + random_image[0])\n    plt.imshow(img)\n    plt.title(target_class)\n    plt.axis(\"off\");\n\n    print(f\"Image shape: {img.shape}\") # show the shape of the image\n\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:48:02.132187Z","iopub.execute_input":"2022-08-03T03:48:02.132564Z","iopub.status.idle":"2022-08-03T03:48:02.140128Z","shell.execute_reply.started":"2022-08-03T03:48:02.132529Z","shell.execute_reply":"2022-08-03T03:48:02.13901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View a random image from the training dataset\nimg = view_random_image(target_dir=\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Train/\",\n                        target_class=\"Culex landing\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:48:02.141771Z","iopub.execute_input":"2022-08-03T03:48:02.142149Z","iopub.status.idle":"2022-08-03T03:48:02.343066Z","shell.execute_reply.started":"2022-08-03T03:48:02.142113Z","shell.execute_reply":"2022-08-03T03:48:02.342111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:48:02.34408Z","iopub.execute_input":"2022-08-03T03:48:02.344426Z","iopub.status.idle":"2022-08-03T03:48:02.353481Z","shell.execute_reply.started":"2022-08-03T03:48:02.34439Z","shell.execute_reply":"2022-08-03T03:48:02.352539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the image shape\nimg.shape # returns (width, height, colour channels)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:48:02.354734Z","iopub.execute_input":"2022-08-03T03:48:02.355794Z","iopub.status.idle":"2022-08-03T03:48:02.364358Z","shell.execute_reply.started":"2022-08-03T03:48:02.355755Z","shell.execute_reply":"2022-08-03T03:48:02.36334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the image shape more closely, you'll see it's in the form (Width, Height, Colour Channels).\n\nIn our case, the width and height vary but because we're dealing with colour images, the colour channels value is always 3. This is for different values of red, green and blue (RGB) pixels.\n\nYou'll notice all of the values in the img array are between 0 and 255. This is because that's the possible range for red, green and blue values.\n\nFor example, a pixel with a value red=0, green=0, blue=255 will look very blue.\n\nSo when we build a model to differentiate between our images of the 6 classes, it will be finding patterns in these different pixel values which determine what each class looks like.","metadata":{}},{"cell_type":"code","source":"# Get all the pixel values between 0 & 1\nimg/255. ","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:48:02.365822Z","iopub.execute_input":"2022-08-03T03:48:02.366255Z","iopub.status.idle":"2022-08-03T03:48:02.379501Z","shell.execute_reply.started":"2022-08-03T03:48:02.366199Z","shell.execute_reply":"2022-08-03T03:48:02.378434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the seed\ntf.random.set_seed(42)\n\n# Preprocess data (get all of the pixel values between 1 and 0, also called scaling/normalization)\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nvalid_datagen = ImageDataGenerator(rescale=1./255)\n\n# Setup the train and test directories\ntrain_dir = \"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Train/\"\nval_dir =\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Pred/\"\ntest_dir = \"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Test/\"\n\n# Import data from directories and turn it into batches\ntrain_data = train_datagen.flow_from_directory(train_dir,\n                                               batch_size=32, # number of images to process at a time \n                                               target_size=(224, 224), # convert all images to be 224 x 224\n                                               class_mode=\"categorical\", # type of problem we're working on\n                                               seed=42)\n\nvalid_data = valid_datagen.flow_from_directory(val_dir,\n                                               batch_size=32,\n                                               target_size=(224, 224),\n                                               class_mode=\"categorical\",\n                                               seed=42)\n\n# Create a CNN model (same as Tiny VGG - https://poloclub.github.io/cnn-explainer/)\nmodel_1 = tf.keras.models.Sequential([\n  tf.keras.layers.Conv2D(filters=10, \n                         kernel_size=3, # can also be (3, 3)\n                         activation=\"relu\", \n                         input_shape=(224, 224, 3)), # first layer specifies input shape (height, width, colour channels)\n  tf.keras.layers.Conv2D(64, 3, activation=\"relu\"),\n  tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2)\n                            padding=\"valid\"), # padding can also be 'same'\n  tf.keras.layers.Conv2D(128, 3, activation=\"relu\"),\n  tf.keras.layers.Conv2D(512, 3, activation=\"relu\"), # activation='relu' == tf.keras.layers.Activations(tf.nn.relu)\n  tf.keras.layers.MaxPool2D(2),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(6, activation=\"softmax\") # binary activation output\n])\n\n# Compile the model\nmodel_1.compile(loss= tf.keras.losses.CategoricalCrossentropy(),\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_1 = model_1.fit(train_data,\n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=valid_data,\n                        validation_steps=len(valid_data))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:48:02.38104Z","iopub.execute_input":"2022-08-03T03:48:02.381643Z","iopub.status.idle":"2022-08-03T03:50:59.13304Z","shell.execute_reply.started":"2022-08-03T03:48:02.381608Z","shell.execute_reply":"2022-08-03T03:50:59.132078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After 5 epochs, our baseline score are 70.67% accuracy on training data and  68.67% accuracy on the validation data.","metadata":{}},{"cell_type":"code","source":"# Check out the layers in our model\nmodel_1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:50:59.13764Z","iopub.execute_input":"2022-08-03T03:50:59.137937Z","iopub.status.idle":"2022-08-03T03:50:59.145649Z","shell.execute_reply.started":"2022-08-03T03:50:59.137911Z","shell.execute_reply":"2022-08-03T03:50:59.144412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize data (requires function 'view_random_image' above)\nplt.figure()\nplt.subplot(1, 2, 1)\nCulex_landing = view_random_image(\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Train/\", \"Culex landing\")\nplt.subplot(1, 2, 2)\nCulex_smashed = view_random_image(\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Train/\", \"Culex smashed\")\n","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:50:59.147372Z","iopub.execute_input":"2022-08-03T03:50:59.148113Z","iopub.status.idle":"2022-08-03T03:50:59.361143Z","shell.execute_reply.started":"2022-08-03T03:50:59.148078Z","shell.execute_reply":"2022-08-03T03:50:59.360185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(1, 2, 1)\naegypti_landing = view_random_image(\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Train/\", \"aegypti landing\")\nplt.subplot(1, 2, 2)\naegypti_smashed = view_random_image(\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Train/\", \"aegypti smashed\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:50:59.362458Z","iopub.execute_input":"2022-08-03T03:50:59.363636Z","iopub.status.idle":"2022-08-03T03:50:59.571781Z","shell.execute_reply.started":"2022-08-03T03:50:59.363597Z","shell.execute_reply":"2022-08-03T03:50:59.57082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(1, 2, 1)\nalbopictus_landing = view_random_image(\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Train/\", \"albopictus landing\")\nplt.subplot(1, 2, 2)\nalbopictus_smashed = view_random_image(\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Train/\", \"albopictus smashed\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:50:59.573115Z","iopub.execute_input":"2022-08-03T03:50:59.573581Z","iopub.status.idle":"2022-08-03T03:50:59.788452Z","shell.execute_reply.started":"2022-08-03T03:50:59.573543Z","shell.execute_reply":"2022-08-03T03:50:59.787416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the training curves\npd.DataFrame(history_1.history).plot(figsize=(10, 7));","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:50:59.789681Z","iopub.execute_input":"2022-08-03T03:50:59.790209Z","iopub.status.idle":"2022-08-03T03:51:00.051468Z","shell.execute_reply.started":"2022-08-03T03:50:59.790171Z","shell.execute_reply":"2022-08-03T03:51:00.05041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the validation and training data separately\ndef plot_loss_curves(history):\n    \"\"\"\n    Returns separate loss curves for training and validation metrics.\n    \"\"\" \n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n\n    epochs = range(len(history.history['loss']))\n\n    # Plot loss\n    plt.plot(epochs, loss, label='training_loss')\n    plt.plot(epochs, val_loss, label='val_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.figure()\n    plt.plot(epochs, accuracy, label='training_accuracy')\n    plt.plot(epochs, val_accuracy, label='val_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:51:00.052946Z","iopub.execute_input":"2022-08-03T03:51:00.053926Z","iopub.status.idle":"2022-08-03T03:51:00.062508Z","shell.execute_reply.started":"2022-08-03T03:51:00.053888Z","shell.execute_reply":"2022-08-03T03:51:00.061248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check out the loss curves of model_4\nplot_loss_curves(history_1)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:51:00.064141Z","iopub.execute_input":"2022-08-03T03:51:00.064578Z","iopub.status.idle":"2022-08-03T03:51:00.476451Z","shell.execute_reply.started":"2022-08-03T03:51:00.064543Z","shell.execute_reply":"2022-08-03T03:51:00.475311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"# Create ImageDataGenerator training instance with data augmentation\ntrain_datagen_augmented = ImageDataGenerator(rescale=1/255.,\n                                             rotation_range=0.2,\n                                             zoom_range=0.2,\n                                             width_shift_range=0.2,\n                                             height_shift_range=0.0,\n                                             horizontal_flip=True) # flip the image on the horizontal axis\n\n# Create ImageDataGenerator training instance without data augmentation\ntrain_datagen = ImageDataGenerator(rescale=1/255.) \n\n# Create ImageDataGenerator test instance without data augmentation\nvalid_datagen = ImageDataGenerator(rescale=1/255.)\n\n# Create ImageDataGenerator test instance without data augmentation\ntest_datagen = ImageDataGenerator(rescale=1/255.)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:51:00.478075Z","iopub.execute_input":"2022-08-03T03:51:00.479658Z","iopub.status.idle":"2022-08-03T03:51:00.486545Z","shell.execute_reply.started":"2022-08-03T03:51:00.47962Z","shell.execute_reply":"2022-08-03T03:51:00.485339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import data and augment it from training directory\nprint(\"Augmented training images:\")\ntrain_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n                                                                   target_size=(224, 224),\n                                                                   batch_size=32,\n                                                                   class_mode='categorical',\n                                                                   shuffle=True) # Don't shuffle for demonstration purposes, usually a good thing to shuffle\n\n# Create non-augmented data batches\nprint(\"Non-augmented training images:\")\ntrain_data = train_datagen.flow_from_directory(train_dir,\n                                               target_size=(224, 224),\n                                               batch_size=32,\n                                               class_mode='categorical',\n                                               shuffle=False) \n\nprint(\"Unchanged test images:\")\nval_data = valid_datagen.flow_from_directory(val_dir,\n                                             target_size=(224, 224),\n                                             batch_size=32,\n                                             class_mode='categorical')\ntest_data = test_datagen.flow_from_directory(test_dir,\n                                             target_size=(224, 224),\n                                             batch_size=32,\n                                             class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:51:00.488022Z","iopub.execute_input":"2022-08-03T03:51:00.488489Z","iopub.status.idle":"2022-08-03T03:51:01.440538Z","shell.execute_reply.started":"2022-08-03T03:51:00.488453Z","shell.execute_reply":"2022-08-03T03:51:01.439475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get data batch samples\nimages, labels = train_data.next()\naugmented_images, augmented_labels = train_data_augmented.next() # Note: labels aren't augmented, they stay the same","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:51:01.442189Z","iopub.execute_input":"2022-08-03T03:51:01.442553Z","iopub.status.idle":"2022-08-03T03:51:01.874223Z","shell.execute_reply.started":"2022-08-03T03:51:01.442509Z","shell.execute_reply":"2022-08-03T03:51:01.873188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show original image and augmented image\nrandom_number = random.randint(0, 32) # we're making batches of size 32, so we'll get a random instance\nplt.imshow(images[random_number])\nplt.title(f\"Original image\")\nplt.axis(False)\nplt.figure()\nplt.imshow(augmented_images[random_number])\nplt.title(f\"Augmented image\")\nplt.axis(False);","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:51:01.876299Z","iopub.execute_input":"2022-08-03T03:51:01.877042Z","iopub.status.idle":"2022-08-03T03:51:02.187509Z","shell.execute_reply.started":"2022-08-03T03:51:01.876999Z","shell.execute_reply":"2022-08-03T03:51:02.186354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After going through a sample of original and augmented images, you can start to see some of the example transformations on the training images","metadata":{}},{"cell_type":"code","source":"# Create a CNN model (same as Tiny VGG - https://poloclub.github.io/cnn-explainer/)\nmodel_2 = tf.keras.models.Sequential([\n  tf.keras.layers.Conv2D(filters=10, \n                         kernel_size=3, # can also be (3, 3)\n                         activation=\"relu\", \n                         input_shape=(224, 224, 3)), # first layer specifies input shape (height, width, colour channels)\n  tf.keras.layers.Conv2D(64, 3, activation=\"relu\"),\n  tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2)\n                            padding=\"valid\"), # padding can also be 'same'\n  tf.keras.layers.Conv2D(128, 3, activation=\"relu\"),\n  tf.keras.layers.Conv2D(512, 3, activation=\"relu\"), # activation='relu' == tf.keras.layers.Activations(tf.nn.relu)\n  tf.keras.layers.MaxPool2D(2),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(6, activation=\"softmax\") # categorical activation output\n])\n\n# Compile the model\nmodel_2.compile(loss= tf.keras.losses.CategoricalCrossentropy(),\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_2 = model_2.fit(train_data_augmented,\n                        epochs=5,\n                        steps_per_epoch=len(train_data_augmented),\n                        validation_data=val_data,\n                        validation_steps=len(val_data))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:51:02.188967Z","iopub.execute_input":"2022-08-03T03:51:02.18998Z","iopub.status.idle":"2022-08-03T03:56:31.800511Z","shell.execute_reply.started":"2022-08-03T03:51:02.189937Z","shell.execute_reply":"2022-08-03T03:56:31.799544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check model's performance history training on augmented data\nplot_loss_curves(history_2)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:31.802063Z","iopub.execute_input":"2022-08-03T03:56:31.803642Z","iopub.status.idle":"2022-08-03T03:56:32.331737Z","shell.execute_reply.started":"2022-08-03T03:56:31.803611Z","shell.execute_reply":"2022-08-03T03:56:32.330711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"culex_landing = mpimg.imread(\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Pred/Culex landing/1.jpg\")\nplt.imshow(culex_landing)\nplt.axis(False);","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:32.335999Z","iopub.execute_input":"2022-08-03T03:56:32.338574Z","iopub.status.idle":"2022-08-03T03:56:32.503487Z","shell.execute_reply.started":"2022-08-03T03:56:32.338535Z","shell.execute_reply":"2022-08-03T03:56:32.499449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to import an image and resize it to be able to be used with our model\ndef load_and_prep_image(filename, img_shape=224):\n    \"\"\"\n    Reads an image from filename, turns it into a tensor\n    and reshapes it to (img_shape, img_shape, colour_channel).\n    \"\"\"\n    # Read in target file (an image)\n    img = tf.io.read_file(filename)\n\n    # Decode the read file into a tensor & ensure 3 colour channels \n    # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)\n    img = tf.image.decode_image(img, channels=3)\n\n    # Resize the image (to the same size our model was trained on)\n    img = tf.image.resize(img, size = [img_shape, img_shape])\n\n    # Rescale the image (get all values between 0 and 1)\n    img = img/255.\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:32.504563Z","iopub.execute_input":"2022-08-03T03:56:32.504888Z","iopub.status.idle":"2022-08-03T03:56:32.520451Z","shell.execute_reply.started":"2022-08-03T03:56:32.504855Z","shell.execute_reply":"2022-08-03T03:56:32.518897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load in and preprocess our custom image\nculex_landing = load_and_prep_image(\"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Pred/Culex landing/1.jpg\")\nculex_landing","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:32.522442Z","iopub.execute_input":"2022-08-03T03:56:32.522884Z","iopub.status.idle":"2022-08-03T03:56:32.571243Z","shell.execute_reply.started":"2022-08-03T03:56:32.522851Z","shell.execute_reply":"2022-08-03T03:56:32.570327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add an extra axis\nprint(f\"Shape before new dimension: {culex_landing.shape}\")\nculex_landing = tf.expand_dims(culex_landing, axis=0) # add an extra dimension at axis 0\n#culex_landing = culex_landing[tf.newaxis, ...] # alternative to the above, '...' is short for 'every other dimension'\nprint(f\"Shape after new dimension: {culex_landing.shape}\")\nculex_landing","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:32.57603Z","iopub.execute_input":"2022-08-03T03:56:32.578265Z","iopub.status.idle":"2022-08-03T03:56:32.594897Z","shell.execute_reply.started":"2022-08-03T03:56:32.578227Z","shell.execute_reply":"2022-08-03T03:56:32.593868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a prediction on custom image tensor\npred = model_1.predict(culex_landing)\npred","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:32.596246Z","iopub.execute_input":"2022-08-03T03:56:32.596712Z","iopub.status.idle":"2022-08-03T03:56:32.908318Z","shell.execute_reply.started":"2022-08-03T03:56:32.596676Z","shell.execute_reply":"2022-08-03T03:56:32.90744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust function to work with multi-class\ndef pred_and_plot(model, filename, class_names):\n    \"\"\"\n    Imports an image located at filename, makes a prediction on it with\n    a trained model and plots the image with the predicted class as the title.\n    \"\"\"\n    # Import the target image and preprocess it\n    img = load_and_prep_image(filename)\n\n    # Make a prediction\n    pred = model.predict(tf.expand_dims(img, axis=0))\n\n    # Get the predicted class\n    if len(pred[0]) > 1: # check for multi-class\n        pred_class = class_names[pred.argmax()] # if more than one output, take the max\n    else:\n        pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round\n\n    # Plot the image and predicted class\n    plt.imshow(img)\n    plt.title(f\"Prediction: {pred_class}\")\n    plt.axis(False);","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:32.921312Z","iopub.execute_input":"2022-08-03T03:56:32.92189Z","iopub.status.idle":"2022-08-03T03:56:32.929318Z","shell.execute_reply.started":"2022-08-03T03:56:32.921855Z","shell.execute_reply":"2022-08-03T03:56:32.928363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test our model on a custom image\npred_and_plot(model_1, \"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Pred/Culex landing/1.jpg\", class_names)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:32.930805Z","iopub.execute_input":"2022-08-03T03:56:32.931424Z","iopub.status.idle":"2022-08-03T03:56:33.131642Z","shell.execute_reply.started":"2022-08-03T03:56:32.931389Z","shell.execute_reply":"2022-08-03T03:56:33.130773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice! Our model got the prediction right.","metadata":{}},{"cell_type":"code","source":"pred_and_plot(model_1, \"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Pred/albopictus smashed/10.jpg\", class_names)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:33.133505Z","iopub.execute_input":"2022-08-03T03:56:33.134873Z","iopub.status.idle":"2022-08-03T03:56:33.428671Z","shell.execute_reply.started":"2022-08-03T03:56:33.134835Z","shell.execute_reply":"2022-08-03T03:56:33.427704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice! Our model got the prediction right.","metadata":{}},{"cell_type":"code","source":"# Save a model\nmodel_1.save(\"saved_trained_model\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:33.433177Z","iopub.execute_input":"2022-08-03T03:56:33.433662Z","iopub.status.idle":"2022-08-03T03:56:34.959398Z","shell.execute_reply.started":"2022-08-03T03:56:33.433616Z","shell.execute_reply":"2022-08-03T03:56:34.958245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load in a model and evaluate it\nloaded_model_11 = tf.keras.models.load_model(\"saved_trained_model\")\nloaded_model_11.evaluate(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:34.962971Z","iopub.execute_input":"2022-08-03T03:56:34.963318Z","iopub.status.idle":"2022-08-03T03:56:56.25784Z","shell.execute_reply.started":"2022-08-03T03:56:34.963289Z","shell.execute_reply":"2022-08-03T03:56:56.25692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transfer Learning with TensorFlow with Fine-tuning","metadata":{}},{"cell_type":"code","source":"# Create data inputs\nimport tensorflow as tf\nIMG_SIZE = (224, 224) # define image size\ntrain_data = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n                                                                            image_size=IMG_SIZE,\n                                                                            label_mode=\"categorical\", # what type are the labels?\n                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\nval_data = tf.keras.preprocessing.image_dataset_from_directory(directory=val_dir,\n                                                                           image_size=IMG_SIZE,\n                                                                           label_mode=\"categorical\")\ntest_data = tf.keras.preprocessing.image_dataset_from_directory(directory=val_dir,\n                                                                           image_size=IMG_SIZE,\n                                                                           label_mode=\"categorical\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:56.259472Z","iopub.execute_input":"2022-08-03T03:56:56.259849Z","iopub.status.idle":"2022-08-03T03:56:57.082153Z","shell.execute_reply.started":"2022-08-03T03:56:56.259813Z","shell.execute_reply":"2022-08-03T03:56:57.081123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.class_names","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:57.083493Z","iopub.execute_input":"2022-08-03T03:56:57.084082Z","iopub.status.idle":"2022-08-03T03:56:57.091348Z","shell.execute_reply.started":"2022-08-03T03:56:57.084044Z","shell.execute_reply":"2022-08-03T03:56:57.090295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See an example batch of data\nfor images, labels in train_data.take(1):\n      print(images, labels)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:57.092781Z","iopub.execute_input":"2022-08-03T03:56:57.093962Z","iopub.status.idle":"2022-08-03T03:56:57.387544Z","shell.execute_reply.started":"2022-08-03T03:56:57.093926Z","shell.execute_reply":"2022-08-03T03:56:57.386455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Create base model with tf.keras.applications\nbase_model = tf.keras.applications.EfficientNetB0(include_top=False)\n\n# 2. Freeze the base model (so the pre-learned patterns remain)\nbase_model.trainable = False\n\n# 3. Create inputs into the base model\ninputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n\n# 4. If using ResNet50V2, add this to speed up convergence, remove for EfficientNet\n# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n\n# 5. Pass the inputs to the base_model (note: using tf.keras.applications, EfficientNet inputs don't have to be normalized)\nx = base_model(inputs)\n# Check data shape after passing it to base_model\nprint(f\"Shape after base_model: {x.shape}\")\n\n# 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computations)\nx = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\nprint(f\"After GlobalAveragePooling2D(): {x.shape}\")\n\n# 7. Create the output activation layer\noutputs = tf.keras.layers.Dense(6, activation=\"softmax\", name=\"output_layer\")(x)\n\n# 8. Combine the inputs with the outputs into a model\nmodel_3 = tf.keras.Model(inputs, outputs)\n\n# 9. Compile the model\nmodel_3.compile(loss='categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# 10. Fit the model (we use less steps for validation so it's faster)\nhistory_3 = model_3.fit(train_data,\n                        epochs=10,\n                        steps_per_epoch=len(train_data),\n                        validation_data=val_data,\n                        validation_steps=len(val_data))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:56:57.388974Z","iopub.execute_input":"2022-08-03T03:56:57.391652Z","iopub.status.idle":"2022-08-03T03:59:39.586351Z","shell.execute_reply.started":"2022-08-03T03:56:57.39161Z","shell.execute_reply":"2022-08-03T03:59:39.58528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:59:39.587902Z","iopub.execute_input":"2022-08-03T03:59:39.58826Z","iopub.status.idle":"2022-08-03T03:59:39.604762Z","shell.execute_reply.started":"2022-08-03T03:59:39.588223Z","shell.execute_reply":"2022-08-03T03:59:39.603402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check out our model's training curves\nplot_loss_curves(history_3)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:59:39.606812Z","iopub.execute_input":"2022-08-03T03:59:39.607692Z","iopub.status.idle":"2022-08-03T03:59:40.013654Z","shell.execute_reply.started":"2022-08-03T03:59:39.607656Z","shell.execute_reply":"2022-08-03T03:59:40.012421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a data augmentation stage with horizontal flipping, rotations, zooms\ndata_augmentation = keras.Sequential([\n  preprocessing.RandomFlip(\"horizontal\"),\n  preprocessing.RandomRotation(0.2),\n  preprocessing.RandomZoom(0.2),\n  preprocessing.RandomHeight(0.2),\n  preprocessing.RandomWidth(0.2),\n  # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetB0\n], name =\"data_augmentation\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:59:40.016543Z","iopub.execute_input":"2022-08-03T03:59:40.017225Z","iopub.status.idle":"2022-08-03T03:59:40.038758Z","shell.execute_reply.started":"2022-08-03T03:59:40.017181Z","shell.execute_reply":"2022-08-03T03:59:40.037693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View a random image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport os\nimport random\ntarget_class = random.choice(train_data.class_names) # choose a random class\ntarget_dir = \"../input/mosquito-on-human-skin/zw4p9kj6nt-2/data_splitting/Train/\" + target_class # create the target directory\nrandom_image = random.choice(os.listdir(target_dir)) # choose a random image from target directory\nrandom_image_path = target_dir + \"/\" + random_image # create the choosen random image path\nimg = mpimg.imread(random_image_path) # read in the chosen target image\nplt.imshow(img) # plot the target image\nplt.title(f\"Original random image from class: {target_class}\")\nplt.axis(False); # turn off the axes\n\n# Augment the image\naugmented_img = data_augmentation(tf.expand_dims(img, axis=0)) # data augmentation model requires shape (None, height, width, 3)\nplt.figure()\nplt.imshow(tf.squeeze(augmented_img)/255.) # requires normalization after augmentation\nplt.title(f\"Augmented random image from class: {target_class}\")\nplt.axis(False);","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:59:40.040381Z","iopub.execute_input":"2022-08-03T03:59:40.040748Z","iopub.status.idle":"2022-08-03T03:59:40.59227Z","shell.execute_reply.started":"2022-08-03T03:59:40.040713Z","shell.execute_reply":"2022-08-03T03:59:40.591348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup input shape and base model, freezing the base model layers\ninput_shape = (224, 224, 3)\nbase_model = tf.keras.applications.EfficientNetB0(include_top=False)\nbase_model.trainable = False\n\n# Create input layer\ninputs = layers.Input(shape=input_shape, name=\"input_layer\")\n\n# Add in data augmentation Sequential model as a layer\nx = data_augmentation(inputs)\n\n# Give base_model inputs (after augmentation) and don't train it\nx = base_model(x, training=False)\n\n# Pool output features of base model\nx = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n\n# Put a dense layer on as the output\noutputs = layers.Dense(6, activation=\"softmax\", name=\"output_layer\")(x)\n\n# Make a model with inputs and outputs\nmodel_4 = keras.Model(inputs, outputs)\n\n# Compile the model\nmodel_4.compile(loss=\"categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_4 = model_4.fit(train_data,\n                        epochs=10,\n                        steps_per_epoch=len(train_data),\n                        validation_data=val_data,\n                        validation_steps=len(val_data))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T03:59:40.594018Z","iopub.execute_input":"2022-08-03T03:59:40.594719Z","iopub.status.idle":"2022-08-03T04:02:57.399708Z","shell.execute_reply.started":"2022-08-03T03:59:40.59468Z","shell.execute_reply":"2022-08-03T04:02:57.398696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the test data\nresults_1 = model_4.evaluate(test_data)\nresults_1","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:02:57.401241Z","iopub.execute_input":"2022-08-03T04:02:57.401865Z","iopub.status.idle":"2022-08-03T04:03:03.579102Z","shell.execute_reply.started":"2022-08-03T04:02:57.401824Z","shell.execute_reply":"2022-08-03T04:03:03.578083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the loss and accuracy\nplot_loss_curves(history_4)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:03:03.580591Z","iopub.execute_input":"2022-08-03T04:03:03.581218Z","iopub.status.idle":"2022-08-03T04:03:03.991035Z","shell.execute_reply.started":"2022-08-03T04:03:03.58118Z","shell.execute_reply":"2022-08-03T04:03:03.990101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model.trainable = True\n\n# Freeze all layers except for the\nfor layer in base_model.layers[:-10]:\n      layer.trainable = False\n\n# Recompile the model (always recompile after any adjustments to a model)\nmodel_3.compile(loss=\"categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # lr is 10x lower than before for fine-tuning\n              metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:03:03.993338Z","iopub.execute_input":"2022-08-03T04:03:03.994427Z","iopub.status.idle":"2022-08-03T04:03:04.022417Z","shell.execute_reply.started":"2022-08-03T04:03:03.994369Z","shell.execute_reply":"2022-08-03T04:03:04.021428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check which layers are tuneable (trainable)\nfor layer_number, layer in enumerate(base_model.layers):\n      print(layer_number, layer.name, layer.trainable)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:03:04.025058Z","iopub.execute_input":"2022-08-03T04:03:04.025778Z","iopub.status.idle":"2022-08-03T04:03:04.035682Z","shell.execute_reply.started":"2022-08-03T04:03:04.025749Z","shell.execute_reply":"2022-08-03T04:03:04.03466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(model_2.trainable_variables))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:03:04.037124Z","iopub.execute_input":"2022-08-03T04:03:04.038074Z","iopub.status.idle":"2022-08-03T04:03:04.044947Z","shell.execute_reply.started":"2022-08-03T04:03:04.038037Z","shell.execute_reply":"2022-08-03T04:03:04.043944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model saving checkpoints every epoch\ninitial_epochs = 5\nhistory_5 = model_3.fit(train_data,\n                        epochs=initial_epochs,\n                        validation_data=val_data,\n                        validation_steps=len(val_data))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:03:04.04659Z","iopub.execute_input":"2022-08-03T04:03:04.047247Z","iopub.status.idle":"2022-08-03T04:04:25.276342Z","shell.execute_reply.started":"2022-08-03T04:03:04.04721Z","shell.execute_reply":"2022-08-03T04:04:25.275395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fine tune for another 5 epochs\nfine_tune_epochs = initial_epochs + 5\n\n# Refit the model (same as model_3 except with more trainable layers)\nhistory_6 = model_3.fit(train_data,\n                        epochs=fine_tune_epochs,\n                        validation_data=val_data,\n                        initial_epoch=history_5.epoch[-1], # start from previous last epoch\n                        validation_steps=len(val_data))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:04:25.279009Z","iopub.execute_input":"2022-08-03T04:04:25.279371Z","iopub.status.idle":"2022-08-03T04:06:10.651924Z","shell.execute_reply.started":"2022-08-03T04:04:25.279335Z","shell.execute_reply":"2022-08-03T04:06:10.650771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test data\nresults_fine_tune = model_3.evaluate(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:06:10.65571Z","iopub.execute_input":"2022-08-03T04:06:10.656019Z","iopub.status.idle":"2022-08-03T04:06:20.906416Z","shell.execute_reply.started":"2022-08-03T04:06:10.655991Z","shell.execute_reply":"2022-08-03T04:06:20.90543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare_historys(original_history, new_history, initial_epochs=5):\n    \"\"\"\n    Compares two model history objects.\n    \"\"\"\n    # Get original history measurements\n    acc = original_history.history[\"accuracy\"]\n    loss = original_history.history[\"loss\"]\n\n    print(len(acc))\n\n    val_acc = original_history.history[\"val_accuracy\"]\n    val_loss = original_history.history[\"val_loss\"]\n\n    # Combine original history with new history\n    total_acc = acc + new_history.history[\"accuracy\"]\n    total_loss = loss + new_history.history[\"loss\"]\n\n    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n\n    print(len(total_acc))\n    print(total_acc)\n\n    # Make plots\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(total_acc, label='Training Accuracy')\n    plt.plot(total_val_acc, label='Validation Accuracy')\n    plt.plot([initial_epochs-1, initial_epochs-1],\n              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(total_loss, label='Training Loss')\n    plt.plot(total_val_loss, label='Validation Loss')\n    plt.plot([initial_epochs-1, initial_epochs-1],\n              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:06:20.908362Z","iopub.execute_input":"2022-08-03T04:06:20.90877Z","iopub.status.idle":"2022-08-03T04:06:20.920733Z","shell.execute_reply.started":"2022-08-03T04:06:20.908733Z","shell.execute_reply":"2022-08-03T04:06:20.917924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_historys(original_history=history_5, \n                 new_history=history_6, \n                 initial_epochs=5)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:06:20.92249Z","iopub.execute_input":"2022-08-03T04:06:20.923458Z","iopub.status.idle":"2022-08-03T04:06:21.265404Z","shell.execute_reply.started":"2022-08-03T04:06:20.923415Z","shell.execute_reply":"2022-08-03T04:06:21.264323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model.trainable = True\n\n# Freeze all layers except for the\nfor layer in base_model.layers[:-10]:\n      layer.trainable = False\n\n# Recompile the model with data augmentation\nmodel_4.compile(loss=\"categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # lr is 10x lower than before for fine-tuning\n              metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:06:21.266951Z","iopub.execute_input":"2022-08-03T04:06:21.267749Z","iopub.status.idle":"2022-08-03T04:06:21.296871Z","shell.execute_reply.started":"2022-08-03T04:06:21.267712Z","shell.execute_reply":"2022-08-03T04:06:21.295744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model saving checkpoints every epoch\ninitial_epochs = 5\nhistory_7 = model_4.fit(train_data,\n                        epochs=initial_epochs,\n                        validation_data=val_data,\n                        validation_steps=len(val_data))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:06:21.29814Z","iopub.execute_input":"2022-08-03T04:06:21.298513Z","iopub.status.idle":"2022-08-03T04:08:08.528977Z","shell.execute_reply.started":"2022-08-03T04:06:21.298477Z","shell.execute_reply":"2022-08-03T04:08:08.52767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fine tune for another 5 epochs\nfine_tune_epochs = initial_epochs + 5\n\n# Refit the model with data augmentation\nhistory_8 = model_4.fit(train_data,\n                        epochs=fine_tune_epochs,\n                        validation_data=val_data,\n                        initial_epoch=history_5.epoch[-1], # start from previous last epoch\n                        validation_steps=len(val_data))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:08:08.530575Z","iopub.execute_input":"2022-08-03T04:08:08.531555Z","iopub.status.idle":"2022-08-03T04:10:00.405713Z","shell.execute_reply.started":"2022-08-03T04:08:08.531511Z","shell.execute_reply":"2022-08-03T04:10:00.404751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_historys(original_history=history_7, \n                 new_history=history_8, \n                 initial_epochs=5)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:00.407518Z","iopub.execute_input":"2022-08-03T04:10:00.407893Z","iopub.status.idle":"2022-08-03T04:10:00.743701Z","shell.execute_reply.started":"2022-08-03T04:10:00.407857Z","shell.execute_reply":"2022-08-03T04:10:00.742667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions with model\npred_probs = model_4.predict(test_data, verbose=1) # set verbosity to see how long it will take ","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:00.744984Z","iopub.execute_input":"2022-08-03T04:10:00.745938Z","iopub.status.idle":"2022-08-03T04:10:07.042541Z","shell.execute_reply.started":"2022-08-03T04:10:00.745898Z","shell.execute_reply":"2022-08-03T04:10:07.041531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many predictions are there?\nlen(pred_probs)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:07.043997Z","iopub.execute_input":"2022-08-03T04:10:07.044471Z","iopub.status.idle":"2022-08-03T04:10:07.052198Z","shell.execute_reply.started":"2022-08-03T04:10:07.044432Z","shell.execute_reply":"2022-08-03T04:10:07.051102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What's the shape of our predictions?\npred_probs.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:07.053993Z","iopub.execute_input":"2022-08-03T04:10:07.054716Z","iopub.status.idle":"2022-08-03T04:10:07.061625Z","shell.execute_reply.started":"2022-08-03T04:10:07.054681Z","shell.execute_reply":"2022-08-03T04:10:07.060471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How do they look?\npred_probs[:10]","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:07.063213Z","iopub.execute_input":"2022-08-03T04:10:07.063583Z","iopub.status.idle":"2022-08-03T04:10:07.072726Z","shell.execute_reply.started":"2022-08-03T04:10:07.06355Z","shell.execute_reply":"2022-08-03T04:10:07.07153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We get one prediction probability per class\nprint(f\"Number of prediction probabilities for sample 0: {len(pred_probs[0])}\")\nprint(f\"What prediction probability sample 0 looks like:\\n {pred_probs[0]}\")\nprint(f\"The class with the highest predicted probability by the model for sample 0: {pred_probs[0].argmax()}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:07.07462Z","iopub.execute_input":"2022-08-03T04:10:07.075271Z","iopub.status.idle":"2022-08-03T04:10:07.082817Z","shell.execute_reply.started":"2022-08-03T04:10:07.075232Z","shell.execute_reply":"2022-08-03T04:10:07.081717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the class predicitons of each label\npred_classes = pred_probs.argmax(axis=1)\n\n# How do they look?\npred_classes[:10]","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:07.084347Z","iopub.execute_input":"2022-08-03T04:10:07.085Z","iopub.status.idle":"2022-08-03T04:10:07.093564Z","shell.execute_reply.started":"2022-08-03T04:10:07.084964Z","shell.execute_reply":"2022-08-03T04:10:07.092268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note: This might take a minute or so due to unravelling 790 batches\ny_labels = []\nfor images, labels in test_data.unbatch(): # unbatch the test data and get images and labels\n      y_labels.append(labels.numpy().argmax()) # append the index which has the largest value (labels are one-hot)\ny_labels[:10] # check what they look like (unshuffled)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:07.095452Z","iopub.execute_input":"2022-08-03T04:10:07.09671Z","iopub.status.idle":"2022-08-03T04:10:12.225312Z","shell.execute_reply.started":"2022-08-03T04:10:07.096663Z","shell.execute_reply":"2022-08-03T04:10:12.224328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get accuracy score by comparing predicted classes to ground truth labels\nfrom sklearn.metrics import accuracy_score\nsklearn_accuracy = accuracy_score(y_labels, pred_classes)\nsklearn_accuracy","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:12.227334Z","iopub.execute_input":"2022-08-03T04:10:12.228612Z","iopub.status.idle":"2022-08-03T04:10:12.805274Z","shell.execute_reply.started":"2022-08-03T04:10:12.228571Z","shell.execute_reply":"2022-08-03T04:10:12.804287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note: The following confusion matrix code is a remix of Scikit-Learn's \n# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\nimport itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\n# Our function needs a different name to sklearn's plot_confusion_matrix\ndef make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): \n    \n    \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n\n      If classes is passed, confusion matrix will be labelled, if not, integer class values\n      will be used.\n\n      Args:\n        y_true: Array of truth labels (must be same shape as y_pred).\n        y_pred: Array of predicted labels (must be same shape as y_true).\n        classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n        figsize: Size of output figure (default=(10, 10)).\n        text_size: Size of output figure text (default=15).\n        norm: normalize values or not (default=False).\n        savefig: save confusion matrix to file (default=False).\n\n      Returns:\n        A labelled confusion matrix plot comparing y_true and y_pred.\n\n      Example usage:\n        make_confusion_matrix(y_true=test_labels, # ground truth test labels\n                              y_pred=y_preds, # predicted labels\n                              classes=class_names, # array of class label names\n                              figsize=(15, 15),\n                              text_size=10)\n    \"\"\"  \n    # Create the confustion matrix\n    \n    cm = confusion_matrix(y_true, y_pred)\n    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n    n_classes = cm.shape[0] # find the number of classes we're dealing with\n\n    # Plot the figure and make it pretty\n    fig, ax = plt.subplots(figsize=figsize)\n    cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n    fig.colorbar(cax)\n\n    # Are there a list of classes?\n    if classes:\n        labels = classes\n    else:\n        labels = np.arange(cm.shape[0])\n  \n    # Label the axes\n    ax.set(title=\"Confusion Matrix\",\n         xlabel=\"Predicted label\",\n         ylabel=\"True label\",\n         xticks=np.arange(n_classes), # create enough axis slots for each class\n         yticks=np.arange(n_classes), \n         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n         yticklabels=labels)\n  \n    # Make x-axis labels appear on bottom\n    ax.xaxis.set_label_position(\"bottom\")\n    ax.xaxis.tick_bottom()\n\n    ### Added: Rotate xticks for readability & increase font size (required due to such a large confusion matrix)\n    plt.xticks(rotation=70, fontsize=text_size)\n    plt.yticks(fontsize=text_size)\n\n    # Set the threshold for different colors\n    threshold = (cm.max() + cm.min()) / 2.\n\n    # Plot the text on each cell\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        \n        if norm:\n            plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n              horizontalalignment=\"center\",\n              color=\"white\" if cm[i, j] > threshold else \"black\",\n              size=text_size)\n        else:\n            plt.text(j, i, f\"{cm[i, j]}\",\n              horizontalalignment=\"center\",\n              color=\"white\" if cm[i, j] > threshold else \"black\",\n              size=text_size)\n\n  # Save the figure to the current working directory\n    if savefig:\n        fig.savefig(\"confusion_matrix.png\")","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:12.807301Z","iopub.execute_input":"2022-08-03T04:10:12.808124Z","iopub.status.idle":"2022-08-03T04:10:12.822712Z","shell.execute_reply.started":"2022-08-03T04:10:12.808081Z","shell.execute_reply":"2022-08-03T04:10:12.821745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the class names\nclass_names_test = test_data.class_names\nclass_names_test","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:12.823937Z","iopub.execute_input":"2022-08-03T04:10:12.825609Z","iopub.status.idle":"2022-08-03T04:10:12.837817Z","shell.execute_reply.started":"2022-08-03T04:10:12.825572Z","shell.execute_reply":"2022-08-03T04:10:12.836764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot a confusion matrix with all 3600 predictions, ground truth labels and 6 classes\nmake_confusion_matrix(y_true=y_labels,\n                      y_pred=pred_classes,\n                      classes=class_names_test,\n                      figsize=(100, 100),\n                      text_size=20,\n                      norm=False,\n                      savefig=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:12.83971Z","iopub.execute_input":"2022-08-03T04:10:12.840136Z","iopub.status.idle":"2022-08-03T04:10:20.854633Z","shell.execute_reply.started":"2022-08-03T04:10:12.840102Z","shell.execute_reply":"2022-08-03T04:10:20.853761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_labels, pred_classes))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:20.856083Z","iopub.execute_input":"2022-08-03T04:10:20.856746Z","iopub.status.idle":"2022-08-03T04:10:20.878222Z","shell.execute_reply.started":"2022-08-03T04:10:20.856696Z","shell.execute_reply":"2022-08-03T04:10:20.877046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a dictionary of the classification report\nclassification_report_dict = classification_report(y_labels, pred_classes, output_dict=True)\nclassification_report_dict","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:20.879922Z","iopub.execute_input":"2022-08-03T04:10:20.880409Z","iopub.status.idle":"2022-08-03T04:10:20.902283Z","shell.execute_reply.started":"2022-08-03T04:10:20.880354Z","shell.execute_reply":"2022-08-03T04:10:20.901409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create empty dictionary\nclass_f1_scores = {}\n# Loop through classification report items\nfor k, v in classification_report_dict.items():\n    if k == \"accuracy\": # stop once we get to accuracy key\n        break\n    else:\n    # Append class names and f1-scores to new dictionary\n        class_f1_scores[class_names[int(k)]] = v[\"f1-score\"]\nclass_f1_scores","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:20.904145Z","iopub.execute_input":"2022-08-03T04:10:20.90469Z","iopub.status.idle":"2022-08-03T04:10:20.912692Z","shell.execute_reply.started":"2022-08-03T04:10:20.904653Z","shell.execute_reply":"2022-08-03T04:10:20.911657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turn f1-scores into dataframe for visualization\nimport pandas as pd\nf1_scores = pd.DataFrame({\"class_name\": list(class_f1_scores.keys()),\n                          \"f1-score\": list(class_f1_scores.values())}).sort_values(\"f1-score\", ascending=False)\nf1_scores","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:20.914034Z","iopub.execute_input":"2022-08-03T04:10:20.914618Z","iopub.status.idle":"2022-08-03T04:10:20.934Z","shell.execute_reply.started":"2022-08-03T04:10:20.914582Z","shell.execute_reply":"2022-08-03T04:10:20.932983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8, 12))\nscores = ax.barh(range(len(f1_scores)), f1_scores[\"f1-score\"].values)\nax.set_yticks(range(len(f1_scores)))\nax.set_yticklabels(list(f1_scores[\"class_name\"]))\nax.set_xlabel(\"f1-score\")\nax.set_title(\"F1-Scores for 10 Different Classes\")\nax.invert_yaxis(); # reverse the order\n\ndef autolabel(rects): # Modified version of: https://matplotlib.org/examples/api/barchart_demo.html\n  \"\"\"\n  Attach a text label above each bar displaying its height (it's value).\n  \"\"\"\n  for rect in rects:\n    width = rect.get_width()\n    ax.text(1.03*width, rect.get_y() + rect.get_height()/1.5,\n            f\"{width:.2f}\",\n            ha='center', va='bottom')\n\nautolabel(scores)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:20.935255Z","iopub.execute_input":"2022-08-03T04:10:20.936221Z","iopub.status.idle":"2022-08-03T04:10:21.166339Z","shell.execute_reply.started":"2022-08-03T04:10:20.936186Z","shell.execute_reply":"2022-08-03T04:10:21.1653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_and_prep_images(filename, img_shape=224, scale=True):\n    \"\"\"\n      Reads in an image from filename, turns it into a tensor and reshapes into\n      (224, 224, 3).\n\n      Parameters\n      ----------\n      filename (str): string filename of target image\n      img_shape (int): size to resize target image to, default 224\n      scale (bool): whether to scale pixel values to range(0, 1), default True\n    \"\"\"\n    # Read in the image\n    img = tf.io.read_file(filename)\n    # Decode it into a tensor\n    img = tf.io.decode_image(img)\n    # Resize the image\n    img = tf.image.resize(img, [img_shape, img_shape])\n    if scale:\n    # Rescale the image (get all values between 0 and 1)\n        return img/255.\n    else:\n        return img","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:21.169678Z","iopub.execute_input":"2022-08-03T04:10:21.169962Z","iopub.status.idle":"2022-08-03T04:10:21.178636Z","shell.execute_reply.started":"2022-08-03T04:10:21.169935Z","shell.execute_reply":"2022-08-03T04:10:21.177369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make preds on a series of random images\nimport os\nimport random\n\nplt.figure(figsize=(17, 10))\nfor i in range(3):\n  # Choose a random image from a random class \n    class_name = random.choice(class_names)\n    filename = random.choice(os.listdir(test_dir + \"/\" + class_name))\n    filepath = test_dir + class_name + \"/\" + filename\n\n    # Load the image and make predictions\n    img = load_and_prep_images(filepath, scale=False) # don't scale images for EfficientNet predictions\n    pred_prob = model_4.predict(tf.expand_dims(img, axis=0)) # model accepts tensors of shape [None, 224, 224, 3]\n    pred_class = class_names[pred_prob.argmax()] # find the predicted class \n\n    # Plot the image(s)\n    plt.subplot(1, 3, i+1)\n    plt.imshow(img/255.)\n    if class_name == pred_class: # Change the color of text based on whether prediction is right or wrong\n        title_color = \"g\"\n    else:\n        title_color = \"r\"\n    plt.title(f\"actual: {class_name}, pred: {pred_class}, prob: {pred_prob.max():.2f}\", c=title_color)\n    plt.axis(False);","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:10:21.181294Z","iopub.execute_input":"2022-08-03T04:10:21.182152Z","iopub.status.idle":"2022-08-03T04:10:23.598694Z","shell.execute_reply.started":"2022-08-03T04:10:21.182116Z","shell.execute_reply":"2022-08-03T04:10:23.597665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating learning rate reduction callback\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n                                                 patience=2,\n                                                 verbose=1, # print out when learning rate goes down \n                                                 min_lr=1e-7)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:14:15.08412Z","iopub.execute_input":"2022-08-03T04:14:15.084808Z","iopub.status.idle":"2022-08-03T04:14:15.093521Z","shell.execute_reply.started":"2022-08-03T04:14:15.084769Z","shell.execute_reply":"2022-08-03T04:14:15.092514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nmodel_4.compile(loss=\"categorical_crossentropy\", # sparse_categorical_crossentropy for labels that are *not* one-hot\n                        optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default\n                        metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:14:18.024778Z","iopub.execute_input":"2022-08-03T04:14:18.025142Z","iopub.status.idle":"2022-08-03T04:14:18.042089Z","shell.execute_reply.started":"2022-08-03T04:14:18.025111Z","shell.execute_reply":"2022-08-03T04:14:18.041158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start to fine-tune (all layers)\nhistory_9 = model_4.fit(train_data,\n                                epochs=10, # fine-tune for a maximum of 100 epochs\n                                steps_per_epoch=len(train_data),\n                                validation_data=val_data,\n                                validation_steps=len(val_data), # validation during training on 15% of test data\n                                callbacks=[reduce_lr,early_stopping]) # reduce the learning rate after X epochs of no improvement","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:14:30.113948Z","iopub.execute_input":"2022-08-03T04:14:30.114355Z","iopub.status.idle":"2022-08-03T04:17:22.181272Z","shell.execute_reply.started":"2022-08-03T04:14:30.114322Z","shell.execute_reply":"2022-08-03T04:17:22.180116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the loss and accuracy\nplot_loss_curves(history_9)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-03T04:13:10.614743Z","iopub.execute_input":"2022-08-03T04:13:10.615368Z","iopub.status.idle":"2022-08-03T04:13:11.017202Z","shell.execute_reply.started":"2022-08-03T04:13:10.61533Z","shell.execute_reply":"2022-08-03T04:13:11.016311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see based on the results on many experiments shows that transfer learning without data augmentation(maybe due to squished image in our dataset) has given better accuracy compared to doing training to carry out CNN.On this notebook we use EfficientNet version 0. You can tweak it with another version by looking at the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet) to see how the performance of the metrics. You can also twe another hyperparmater like learning rate,early stopping,and how mny epochs to run.\n\n**NOTE** `Do not carry out TensorBoard callback where kaggle kernel does not support it`","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}